{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "datamining.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMaK0LSYq1l/XaWMkN/KglP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CCTQL/weiqi/blob/master/datamining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukaA9VNvoYhn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch \n",
        "from torchvision import transforms, datasets\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "import os\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xmfvYEsobTb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_transforms = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.RandomAffine(15),\n",
        "    transforms.RandomPerspective(),\n",
        "    # transforms.RandomRotation(15),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,),(0.5,))\n",
        "])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,),(0.5,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST('./data/train', train=True, download=True, transform=train_transforms)\n",
        "test_dataset = datasets.MNIST('./data/test', train=False, download=True, transform=test_transforms)\n",
        "testloader = torch.utils.data.DataLoader(test_dataset, shuffle=False, batch_size=len(test_dataset))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7szX2BqodoA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_teachers = 10\n",
        "\n",
        "\n",
        "# Split the dataset into equal-sized partitions for all the teachers\n",
        "# trainsets = tuple(th.utils.data.random_split(train_dataset, [len(train_dataset)//num_teachers]*num_teachers))\n",
        "idxs = np.random.permutation(len(train_dataset))\n",
        "split_size = len(train_dataset)//num_teachers\n",
        "\n",
        "trainsplits = tuple((train_dataset.data[idxs[i:i+split_size]],train_dataset.targets[idxs[i:i+split_size]]) for i in range(num_teachers))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeQgddTXof5s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TeacherModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TeacherModel, self).__init__()\n",
        "        \n",
        "        # Input shape is (1,28,28) => 784\n",
        "        self.fc1 = nn.Linear(784, 512)\n",
        "        \n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        \n",
        "        self.fc3 = nn.Linear(256,128)\n",
        "        \n",
        "        self.fc4 = nn.Linear(128, 64)\n",
        "        \n",
        "        self.fc5 = nn.Linear(64, 32)\n",
        "        \n",
        "        self.fc6 = nn.Linear(32, 16)\n",
        "        \n",
        "        self.fc7 = nn.Linear(16, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # reshape the data for fc layers\n",
        "        x = x.view(-1, 28*28)\n",
        "        \n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = F.relu(self.fc4(x))\n",
        "        x = F.relu(self.fc5(x))\n",
        "        x = F.relu(self.fc6(x))\n",
        "        \n",
        "        # Get the linear output. Classification is done outside the model.\n",
        "        x = self.fc7(x)\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YD9s53KToilQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainsets = trainsplits\n",
        "teacher_models = list(\n",
        "        # Models are stored in a list since we have to reassign them later\n",
        "        TeacherModel()\n",
        "    for i in range(num_teachers))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVUOBIf4oltT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "retrain = False\n",
        "\n",
        "for i in range(num_teachers):\n",
        "    if not os.path.exists(f'teacher_{i}_chkpt.pth') or retrain:\n",
        "        epochs = 20\n",
        "        trainloader = th.utils.data.DataLoader(trainsets[i], shuffle=True, batch_size=64)\n",
        "        model = teacher_models[i]\n",
        "\n",
        "        criterion = nn.NLLLoss()\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "        for e in range(epochs):\n",
        "            running_loss = 0\n",
        "            steps = 0\n",
        "            for images, labels in trainloader:\n",
        "                images, labels = Variable(images).cuda(), Variable(labels).cuda()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Use log_softmax for local classification\n",
        "                log_ps = F.log_softmax(model(images))\n",
        "                loss = criterion(log_ps, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "                steps += 1\n",
        "\n",
        "                if steps % 20 == 0:\n",
        "                    print(f'Teacher {i}/{num_teachers} | Epoch: {e}/{epochs} | Loss: {np.round(running_loss/steps+1, 3)}')\n",
        "        else:\n",
        "            th.save(model.state_dict(), f'teacher_{i}_chkpt.pth')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4FwuTOMyBSd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "teacher_models = list(\n",
        "        # Models are stored in a list since we have to reassign them later\n",
        "        TeacherModel()\n",
        "    for i in range(num_teachers))\n",
        "\n",
        "for i in range(num_teachers):\n",
        "    chkpt_path = f'teacher_{i}_chkpt.pth'\n",
        "    if os.path.exists(chkpt_path):\n",
        "        state_dict = torch.load(chkpt_path)\n",
        "        teacher_models[i].load_state_dict(state_dict)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNf_DuR1yI4P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opinions = None\n",
        "\n",
        "unlabeled_data, labels = next(iter(testloader))\n",
        "\n",
        "for i in range(num_teachers):    \n",
        "    ps = torch.exp(teacher_models[i](unlabeled_data)) # get teacher's opinion\n",
        "    _, top_class = ps.topk(1, dim=1)\n",
        "      \n",
        "    if opinions is None:\n",
        "        opinions = top_class\n",
        "    else:\n",
        "        opinions = torch.cat((opinions, top_class), dim=1) # concatenate all opinions\n",
        "    \n",
        "    # unlabeled_data = unlabeled_data.get() # retrieve the data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2za_IFLByL3U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def noisy_argmax(x, epsilon=0.1):\n",
        "\n",
        "    # First get the vote count for each datapoint.\n",
        "    count = np.zeros([10000,10])\n",
        "    for i in range(10000):\n",
        "      for j in range(10):\n",
        "        count[i,x[i,j]] += 1 \n",
        "\n",
        "    # Add Laplacian noise to the votecount.\n",
        "    \n",
        "    count = torch.from_numpy(count)\n",
        "    beta = 1 / epsilon\n",
        "    noise = torch.from_numpy(np.random.laplace(0, beta, count.shape))\n",
        "    \n",
        "    n_labels = count.double() + noise\n",
        "    \n",
        "    # Then get the highest votecount index\n",
        "    n_labels = torch.argmax(n_labels, dim=1)\n",
        "    return n_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AsO_cry-Ytn",
        "colab_type": "code",
        "outputId": "91742c95-ce94-412f-fbee-58e6991ba4dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "print(opinions)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[7, 7, 7,  ..., 7, 7, 7],\n",
            "        [2, 2, 2,  ..., 2, 2, 2],\n",
            "        [1, 1, 1,  ..., 1, 1, 1],\n",
            "        ...,\n",
            "        [4, 4, 4,  ..., 4, 4, 4],\n",
            "        [5, 5, 5,  ..., 5, 5, 5],\n",
            "        [6, 6, 6,  ..., 6, 6, 6]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkOKuieqydFp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "noisy_labels = noisy_argmax(opinions, epsilon=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nn7cjidZBK1P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "867efa2b-6ff5-485b-d590-97f168e8ef21"
      },
      "source": [
        "print(noisy_labels)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([7, 2, 1,  ..., 4, 5, 6])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhaYLMcMyQXF",
        "colab_type": "code",
        "outputId": "a9056aa8-d534-4cd9-927e-3d2b9cdbaa78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "equals = labels == noisy_labels\n",
        "accuracy = torch.mean(equals.float())\n",
        "print(f\"Noisy Argmax Accuracy: {int(accuracy*100)}%\")"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Noisy Argmax Accuracy: 95%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}